{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Decision Trees\n",
    "_Author: Kevin Coyle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe how tree based models are created for classification tasks and regression tasks \n",
    "- Gain intuition on the math behind these models\n",
    "- Implement a tree model in Python\n",
    "- Implement a tree model in Python using SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"../figures/decision_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are several algorithms to implement decision trees mathematically. We're using the classification and regression tree (CART) algorithm.\n",
    "Other algorithms that make decision trees:\n",
    "- [ID3](https://medium.com/machine-learning-guy/an-introduction-to-decision-tree-learning-id3-algorithm-54c74eb2ad55)\n",
    "- [C4.5](https://towardsdatascience.com/what-is-the-c4-5-algorithm-and-how-does-it-work-2b971a9e7db0)\n",
    "- [C5.0](https://www.ibm.com/support/knowledgecenter/en/SS3RA7_15.0.0/com.ibm.spss.modeler.help/c50node_general.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Does this mean that trees can perform classification _and_ regression tasks? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Great. Let's go into the math, then the Python, then the SK Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees primarily rely on two equations to split data in a _binary recursive_ fashion:\n",
    "- Gini Impurity\n",
    "- Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall our attempt to classify an animal above. Trees are trying to reduce the possibility of mislabeling an observation, given the data. We can think of it as a question about the features (scroll up to see the example with the animals... ex. \"How big is the animal?\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"The best question is the one that reduces our uncertainty the most.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These two equations are trying to answer two questions:\n",
    "1. Can we quantify how much uncertainty we have at a node (Gini Impurity)?\n",
    "2. How much uncertainty did we just reduce, once we split our data (Information Gain)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Our goal is to measure how much \"uncertainty\" or \"potential to mislabel a target, given the data\"\n",
    "- At the root node, our data has however many classes of labels. In our toy dataset, we have 3 labels.\n",
    "- A node is completely pure at 0 (0 == no uncertainty) and there is more impurity as Gini Impurity rises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine we have a bucket with childrens blocks in the bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"https://i.etsystatic.com/9927164/r/il/d9e833/1518306239/il_1588xN.1518306239_6mf0.jpg\" height=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now imagine we have those 3 blocks, and each block is stamped with the letter \"A\" on every side of the block, and those 3 blocks with the letter A are all in one bucket. Also, we have a hat that has 3 pieces of paper (our labels) in the hat, and each piece of paper has a label written on it, that says \"A.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pull a block out of the bucket and pull a label out of the hat, we will always obtain the correct letter. There are 3 blocks, and there are 3 corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our Proportion (p) - in other words, our \"ratio\" - is 3/3 (or 1). We have no way to make a mistake here, so our impurity is $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally defined:\n",
    "\n",
    "Gini Impurity = \n",
    "$\\sum_{j=1}^{c} = 1 - p_i$\n",
    "\n",
    "Let's implement this math with our example of the blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Great. Now we know that Gini Impurity is how we classify how \"pure\" or \"impure\" our node is, given the number of labels (also called: \"classes\"), and our proportion of data within those classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Would a Gini Impurity score of 0.2 be more pure or less pure than a score of 0.21? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try this again with our example of 3 blocks, and this time, assume that we have 1 block for 3 letters: A, B, and C (iow: 1 bucket has 3 blocks in it and each block as a different letter on it).\n",
    "\n",
    "Now in the hat, we have 3 pieces of paper. One piece of paper says \"A\" on it, the second piece of paper says \"B\" on it, and the third piece of paper says \"C\" on it.\n",
    "\n",
    "Calculate this, with the Gini Impurity formula from above, and our _new proportion_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that we're trying to answer two questions to determine the \"best split\" in our data:\n",
    "1. How much uncertainty do we have in our node at a current point in time?\n",
    "2. If we make a split, how much did we just reduce our uncertainty by?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This second question is \"information gain.\" Rolled up into this one question though is an inherent sub question:\n",
    "\"How much uncertainty was there to begin with?\"\n",
    "\n",
    "\n",
    "To solve this sub question, we turn to a new formula called \"Entropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Paraphrashing from our compatriots in mathematical arms, the physicists, we can loosely define Entropy as _\"an amount of disorder.\"_\n",
    "\n",
    "Here, we can rephrase it as \"entropy is the amount of messiness in our data classification.\" \n",
    "<img src = \"https://c8.alamy.com/comp/AB6DGB/extremely-messy-room-of-a-teenage-AB6DGB.jpg\" height=400 width=400>\n",
    "(lots of entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal is to sort/tidy this data. Entropy is highly related to Gini Impurity, but they are calculated differently and have different objectives (mostly having to do with the fact that we want to find an average weight of entropy because we care more about a large set with a low set of uncertainty, than a small set with a high amount of uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally defined.\n",
    "\n",
    "$Entropy:\\; H(C) = -\\sum p(C)\\log p(C)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we know how much entropy exists in our node, we can calculate how much a split would effect our information gain. We'll keep track of this amount, and the split that gives us the most gain is our best split (or \"is the best question to reduce our uncertainty\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This second half of the equation in Information Gain is could also be called [conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy) iow: given this new split, how much entropy are we expected to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally defined.\n",
    "\n",
    "$Information\\ Gain:\\; I(C,Y)= H(C)-H(C|Y)$\n",
    "\n",
    "or\n",
    "\n",
    "Information Gain = entropy(the parent node) - the weighted average * entropy(the child node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weighted average can be found like:\n",
    "Weighted avg] Entropy(children) = \n",
    "\n",
    "$(Number\\ of\\ examples\\ in\\ left\\ child\\ node)\\ /\\ (Total\\ number\\ of\\ examples\\ in\\ parent\\ node)\\ *\\ (entropy\\ of\\ left\\ node)$ \n",
    "\n",
    "+\n",
    "\n",
    "$(Number\\ of\\ examples\\ in\\ right\\ child\\ node)\\ /\\ (Total\\ number\\ of\\ examples\\ in\\ parent\\ node)\\ *\\ (entropy\\ of\\ right\\ node)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What question is information gain trying to answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees in SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal is to always understand our models and what is happening within them, down to the code.\n",
    "\n",
    "However, it is often prudent to stand on the shoulders of giants for our initial prototyping. Let's learn how to use SK Learn's Decision Tree classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Import our data. Today, we're predicting the outcome of a March Madness game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ncaa_df = pd.read_csv('../data/ncaa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaa_df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Check out the dtypes and summary statistics for data. Also, are there any NaNs? Handle those if so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's drop the Rank column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ncaa_df.drop(columns=['Rank'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to change the Location into a number. To acheive:\n",
    "- Use Pandas get dummies to create a new dataframe called \"location_dummies.\"\n",
    "- You can drop one of the locations if you wish (recall: multicollinearity)\n",
    "- Join this location_dummies back onto our dataframe, then drop the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create an `X`and a `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = ['Seed', 'STL', 'PPG_Diff', 'PPG']\n",
    "X = ncaa_df[feature_cols]\n",
    "y = ncaa_df.Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncaa_df.Result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Train test split that data! First, import your train_test_split function in sklearn.model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Import Decision Tree Classifier from SkLearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instantiate our model. Also, let's check out some of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that sklearn has some excellent default settings. One of these is Gini! Write out a couple of the hyper parameters in the cell below. We're going to go over them in a moment, so don't worry if you don't know what these do yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's save these predictions to a variable called Å· (y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(y_pred - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall there were several hyperparameters available to us in sklearn. We're now going to focus on a few important ones. To set a hyper parameter in SK Learn, you need to instantiate the model with that hyper parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Seed', 'STL', 'PPG_Diff', 'PPG']\n",
    "X = ncaa_df[feature_cols]\n",
    "y = ncaa_df.Result\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#reinstantiate model\n",
    "DT_2 = DecisionTreeClassifier(min_samples_split=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#fit that model\n",
    "DT_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#check results\n",
    "y_pred = DT_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(y_pred - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees can be great models, and they form the basis of more robust, powerful models. We'll go over other tree based models at a later point in class (like bagged trees and random forests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Their real distinction though, is their interpretability. You know what decision trees look like if you craft them by hand... but what if you could do that with the model you just fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can! Graphviz will allow us to do this. You'll create a new file type: a dot file. Then you convert this to an image file and you're golden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!conda install graphviz --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`export_graphviz` is a function in sklearn's tree module... import it!\n",
    "\n",
    "Then:\n",
    "\n",
    "import graphviz, which will allow us to change the dot file into a PDF or PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following code exports our tree as dot file, and then converts it back to a png!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(dec_tree, out_file=None, \n",
    "                     feature_names= X.columns,  \n",
    "                     class_names= \"result\", \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "#this last line creates a PDF of your diagram. You can change the format to \"png\" if you want an image\n",
    "graph.render(\"ncaa\", format='png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cool! Now you can show your friends how you chose your NCAA bracket and have some machine learning to back your analysis. :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What does \"entropy\" refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Can you name one of the two hyper parameters we discussed for decision trees (it's okay to look if you can't remember!)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Can you name the other one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### I have 4 items: a basketball, a soccer ball, a frisbee, and a tennis ball in a bucket and 4 labels of each of those in another container. If I pull 1 item from the bucket and 1 item from the other container, what is the ratio/proportionality that I may choose the correct label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of trees:\n",
    "- Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n",
    "- Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.\n",
    "- Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Disadvantages of trees:\n",
    "- Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\n",
    "- Additionally, trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved. We introduce these concepts in a later lesson!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
